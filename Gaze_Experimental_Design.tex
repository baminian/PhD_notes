\documentclass[11pt,a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{verbatim}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{amsmath}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\med}{med}

\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{lmodern}
\usepackage{empheq}
\usepackage{epsfig}
\usepackage{tikz}
\usepackage{xcolor}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{fancyvrb}
\usepackage{moreverb}
\usepackage{listings}
\usepackage{url}
%%\usepackage[round]{natbib}

\setlength{\unitlength}{1mm}
\usepackage{pstricks}

\usepackage[top=3cm, bottom=3cm, left=3cm, right=3cm]{geometry}

%%\usepackage{hyperref}

\begin{document}
\section{Introduction}
While many algorithm for learning from demonstration are based on training data, we propose to leverage the interaction between the teacher and learner. We believe that learning from demonstration with natural interaction has many advantages. It allows easy reprogramming of the robot since data acquisition in laboratory framework is not necessary. Moreover, the robot can interrupt the demonstration and engage in an interaction with the teacher in order to disambiguate its understanding, reducing the amount of data needed for learning.
\newline
In order to allow natural interaction, we rely on non-intrusive recording systems. Teaching from demonstration involves many communication cues. The language informed about the objects and the task to perform nevertheless other non-verbal communication cues carry those informations too. In particular, the gaze is an important one. 
\newline
The gaze informs about attention and intention. Therefore, it can be used as a prior for task segmentation and recognition. The estimation of the gaze relies on the algorithm presented in \cite{Funes2016} where the gaze is infrared from RGB and Depth video steams obtained from a Microsoft Kinect.

\section{Motivation}
The present dataset is used to evaluate the gaze estimation algorithm. As a first step, we aim to evaluate the accuracy of the gaze estimation algorithm using a controlled (supervised) calibration procedure. Furthermore, we will use the human-robot interactions to define an unsupervised calibration method. In particular, we are interested in calibration methods using interaction cues to infer targets of calibration. As a second step, we aim to leverage gaze behavior during demonstration. In particular, the segmentation and recognition of the demonstration. For more details on the proposed benchmarks refer to section~\ref{Benchmarks}.
\subsection{Gaze calibration}
\paragraph{Supervised}: The gaze calibration procedure relies on the definition of targets for calibration. In the case of point (marker) targets, the target is trivially defined. The present data set aims to provide enough calibration points to evaluate the gaze estimation algorithm with a calibration procedure. Nevertheless, the inference of the gaze target is less trivial when the object of attention is big with respect to some dimensions. We will be interesting to evaluate the effect of the gaze calibration procedure while the target is an object (of various size and shape).
\paragraph{Unsupervised}: Interaction cues can be used for the definition of targets for calibration. In \cite{Siegfried2017}, the authors rely on the conversational interaction between the people; we look at the person we talk to. While performing object related actions, the gaze fixation at the object usually precedes the grasping of the object and the fixation at the landing position precede the placing \cite{Land2009} (and more).

\subsection{Gaze behavior}
Based on the accuracy of the gaze estimation algorithm, we would like to leverage some gaze behaviors. In particular, we evaluate how well the gaze estimation algorithm can inferred the Visual Focus Of Attention (VFOA). Further, we would like to incorporate the VFOA model in a multi-modal task segmentation and recognition methods. In particular, we are interesting in the classification of the VFOA as reactive and proactive in order to segment and recognize demonstrations.

\section{Data collection}
\label{Data_Collection}
In this section, we first describe our recording methodology and then describe the different recording sessions constituting the dataset.
\subsection{Overview}
\label{Overview}
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.05, angle=-90]{Pictures/Baxter_Setup.jpg}
\end{center}
\caption{Experimental setup. \label{Experimental_Setup}}
\end{figure}
The recording setup is as shown in figure \ref{Experimental_Setup}. It comprises a RGB-D camera (a Microsoft Kinect v2), a robot (a Baxter from Rethink Robotics), a table with marker, a cup, a saucer,.... The characteristics and purpose or function of each element are as follow:
\begin{itemize}
\item Microsoft Kinect v2: this consumer device provides standard video (RGB) and Depth video steams. The RGB camera has an HD resolution ($1920 \times 1080$) while the Depth camera has a resolution of $512 \times 424$. The acquisition rate is 30 frame-per-second.
\item Baxter robot: this is a human like robot from Rethink Robotics. We will use the robot's ego-motion to infer gaze target. In particular, the position of the head of the robot (screen) and its end-effectors are recored.
\item Table with markers: those markers are used as calibration targets for the gaze. Moreover, the object are placed on those markers in order to infer there position without the need of an object detection algorithm. The markers are numbered in order to differentiate them.
\item Cup and Saucer: those object are involved in the demonstration task. We provide a 3D mesh of this objects.
\item Other Objects: those objects are used as gaze target for calibration. We provide a 3D mesh for each objects.
\end{itemize}

\subsection{Recording sessions}
In this section, the various experiments are described.
\subsubsection{Static-pose and end-effector target (SP-ET)}
The end-effector of the robot is used as a target for calibration. More precisely, a gripper of the robot is marked with a white rubber band as shown on figure XXX. The participant is asked to look at the camera (kinect) on the chest of the robot in order to guarantee, more or less, a frontal image of the face. Once the participant is ready, it will be asked to keep his head pose fixed during the calibration procedure. The latter consists in ...
\paragraph{Motivation}
The algorithm for gaze estimation is based on the evaluation of the head pose. Indeed, based on the head pose, a frontal image of the face is computed. In order to avoid errors of the gaze estimation due to the frontalization procedure, a static head pose is imposed. We are interested in the evaluation of the gaze estimation algorithm independantly of the frontalization procedure. In particular, how calibration is affected. (not clear)

\subsubsection{Natural-pose and end-effector target (NP-ET)}
The end-effector of the robot is used as a target for calibration as described in the precedent recording session (SP-ET). Nevertheless, in the present recording session, the participant is free to move his head. The calibration procedure consists in ...
\paragraph{Motivation}
We are interested of testing our calibration algorithm in a more natural setup. In particular, we want to test how the head pose may affect the calibration. Moreover, the calibration of the gaze tracker using the SP-ET and NP-ET recording sessions will be used to evaluate our unsupervised calibration method based on interactions.

\subsubsection{Markers on the table targets (MT)}
In this recording session, the numbered markers on the table are used as targets for calibration (see figure XXX). The participant starts by looking at the robot face (screen). The robot asks: 
\begin{center}
"\textit{Can you see the marker number \texttt{XXX}?}" 
\end{center}
And wait for the response. Then it asks: 
\begin{center}
\textit{Can you show it to me, please?}"
\end{center}
 Repeat 5 times.
\paragraph{Motivation.}
The MT recording session provides additionnal points for calibration with natural head pose. In particular, the calibration points are contains in the working region for the future recording sessions. Beside, the supervised calibration of our gaze tracker, we are interested in the gaze behavior during interaction. The MT recording provides to type of interaction cues. The first question may produce a gaze at the marker. The interaction can be split in 4 steps: listen, search, find, answer. That will be intersting to recognize for our future research. The second question produce another type of interaction involving pointing gesture. We believe that gaze and pointing  are strongly correlated and may give a strong prior for an unsupervised calibration of the gaze tracker.
 
\subsubsection{Object on the table target (OT)}
In this recording, some objects are placed on the marker of the table. The participant is facing the robot, waiting for interaction. The robot asks: 
\begin{center}
"\textit{Where is the \texttt{<object>}?}"
\end{center}
And wait for an answer. Repeat 5 times.
\paragraph{Motivation.} The present recording session is quiet similar to the MT recoding. Nevertheless, it differs with respect to two aspects. First, the points of fixation are not as trivial as in the case of markers on the table. In particular, we are intersting in the estimation of calibration algorithm for different size and shape of objects. Moreover, the OT recording provides a slighty different interaction. The participant is more free to choose his way of explaining "\textit{where is the \texttt{<object>}}". Some participants may prefer to point to the object or only look at the object. While some others will refer to the position of the object with respect to the table or the other objects on the table. This aspect of interaction will be interested for our future research. 


\subsubsection{Put on, then move on (Pon-Mon)}
The participant takes a card randomly from the desk. On the card, A tasks is described. The task is given by:
\begin{center}
\textit{Put the \texttt{<object1>} on the \texttt{<object2>}. Then, move the \texttt{<object2>} (with \texttt{<object1>} remaining on) on marker number \texttt{XXX}.}
\end{center}
\paragraph{Motiviation.} Through this recording session, we aim to use interaction cues for an unsupervised calibratio procedure. The demonstration consists in two pick an place. Using gaze prior as given in \cite{Land2009}, we aim to test our calibration procedure. Moreover, we are intersting in task segmentation and recognition. In particular, we want to know if the gaze estimation algorithm can capture a difference in the gaze behavior between the two pick-and-place demonstrations. Moreover, we will experiment different size and shape of objects and initial position (distance between the objects may influence the calibration procedue).

\subsubsection{Put on, then throw in (Pon-Ton)}
The participant takes a card randomly from the dek. On the card, a tasks is described. The task is given by:
\begin{center}
\textit{Put the marble on/in \texttt{<object>}. Then, throw the marble (without touching it, that is, using only the \texttt{<object>}) in the \texttt{<container>}}
\end{center}
\paragraph{Motivation.} The present recording session is slightly similar to the Pon-Mon recording. In the Pon-Ton recording session the first object is always the marble. The latter is clear point and the region of attention can be defined accuretely. The second step of the recording session involves a different dynamic than the Pon-Mon recording. In particular, we want to know if the gaze estimation algorithm can capture any difference in gaze behavior between the different object involved, as well as, between the Pon-Mon recording.

\subsubsection{Set a table (ST)}
The participant is asked to set a table (fork, knife, spoon, dish, glass). The objects are randomly placed on the markers of the table.
\paragraph{Motivation.}
The present recording session involves a more natural demonstration of a task. The number of object involved in the interaction are higher than the previous recording sessions. Moreover, the task is segmented in more subtask. We are interested to know if we can calibrate our gaze estimation algorithm through this interaction, as well as, segmented and recognition of the task presented.



\section{Data processing}
Besides the raw data describe in section \ref{Overview}, we also provide additional information that is essential for deriving ground truth measures for evaluation or simply useful to exploit the dataset and run the experiments. It compromises the camera calibration, the synchronization, the head pose, the manual annotations, the target position, and 3D object meshes. Below, we present the procedure to estimate those parameters

\subsection{World coordinate system definition}
To standardize the definition all 3D variable in the data, we have defined a common world coordinate system (WCS), in which the variable refers to \textit{meters}. It has been define as the robot frame being at the base of the robot.

\subsection{RGB-D sensors intrinsic calibration}
For the calibration of the Microsoft Kinect v2, we rely on the \texttt{iai\_kinect} softwares. It computes the intrinsic of the RGB and Depth camera, as well as the extrinsic parameters between both cameras.

\subsection{RGB-D sensors extrinsic calibration and synchrony}
The rigid transform from the robot to the camera frame can be evaluated using the position of the end-effector in the RGB-D camera. The synchronization of the robot and camera use done by time-stamping the robot time with the computer and using the \texttt{MessageFilter} ROS package.

\subsection{Target position}
By placing the object on the markers of the table, one can evaluate there position when there are on the table. In the future, we will provide object position using our 3D meshes of the object or an other methods based on object detection. Nevertheless, we have to provide a ground truth for the gaze during OT (in particular). We ask to the participants which part of the object they look at. We report those region on pictures of the object. The position of the target is given for each frame. Each object has its own ID which is given in the data set. Moreover, an additional variable is set to "left", "right" or "NA" if the object is manipulated, respectively, by the left, the right hand or is not manipulated.

\subsection{Body landmark}
We will report body landmark position as hands, shoulders and head pose using our tracker (REFERENCE).

\subsection{Gaze behavior label}
Each frame is labeled with a "b" when the participant is blinking. The frame is labeled by the object ID when the participant is looking at the object. In the case of saccade or undefined VFOA (gaze search), the frame is labeled with "NA". This labeling is done manually. 
\paragraph{Remark}
For the RT recording, the target is not visible in the RGB stream. Therefore, it is difficult to label the gaze at the end-effector. One could to the labelling during the recording when the supervisor has a complete view of the scene. Another possibility consists in recording again if the participant has been distracted. The check is done during the recording.

\section{Benchmarks}
\label{Benchmarks}
In present section, we describe different evaluation benchmarks for gaze calibration, as well as for gaze behavior and task segmentation and recognition.
\subsection{Benchmark 1: Natural versus Static head pose activity}
\subsection{Benchmark 2: RT versus MT} 
\subsection{Benchmark 3: MT versus OT}
\subsection{Benchmark 4: RT versus OT}
\subsection{Benchmark 5: RI versus RT, MT and OT}
\subsection{Benchmark 6: Task segmentation and recognition (CI, TI)}
In our experiments, the tasks consist of pick and place actions. We will characterize a pick and place action by: the object ID, the final position (Gaussian) of the object, a boolean that state if the orientation of the object matters when carrying it, and the timestamps of each step.
\paragraph{Remark} The object position while manipulated is given by the hand position.

\section{Unsupervised gaze calibration}
\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{Pictures/GazeObjectBehavior_Land.png}
\end{center}
\caption{Results from \cite{Land2009}. (\textbf{A}) The relative timings of body movements, fixations on particular objects, and manipulation of those objects by the hands during the first 70 s of a tea-making session.
(\textbf{B}) Average relative timings of trunk movements,
fixations on particular objects (which may involve several individual fixations), and manipulation of those objects. The data represent
94 object-related actions from three subjects. Typical actions last about 3 s (extended waiting periods, such as watching for the kettle to
boil, have been excluded). Note that trunk movements (when present) precede fixations, which precede manipulation, by about 0.6 s in
each case. Similarly, fixation moves on to the next object about 0.6 s before manipulation is complete. There are wide variations in timing
(histograms), with manipulation sometimes preceding gaze fixation.\label{LandResults}}
\end{figure}

\begin{figure}
\includegraphics[scale=0.4]{Pictures/MHRI_Point.png}
\end{figure}

\clearpage
\bibliographystyle{ieeetr}
\bibliography{Notes}

\end{document}